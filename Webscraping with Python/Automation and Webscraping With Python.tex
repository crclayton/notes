\documentclass{article}
\usepackage{array, booktabs, graphicx, apacite, setspace, tocbibind, lipsum}
\usepackage[titletoc,toc,title]{appendix}
\usepackage[toc,section=section]{glossaries}
\usepackage[parfill]{parskip}

% format appendix numbering
\renewcommand\appendix{\par
  \setcounter{section}{0}
  \setcounter{subsection}{0}
  \setcounter{figure}{0}
  \setcounter{table}{0}
  \renewcommand\thesection{Appendix \Alph{section}}
  \renewcommand\thefigure{\Alph{section}\arabic{figure}}
  \renewcommand\thetable{\Alph{section}\arabic{table}}
}

% bold typewriter
\usepackage{lmodern}
\ttfamily
\fontseries{b}\selectfont

% renames "Contents" to "Table of Contents"
\renewcommand\contentsname{Course Overview}


% define subsubsubsection
\usepackage{titlesec}
\usepackage{hyperref}


\titleclass{\subsubsubsection}{straight}[\subsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}



\usepackage[top=1.5in, bottom=1.9in, left=1.75in, right=1.75in]{geometry}


\begin{document}

% ------------------------------ %
% -------- FRONT MATTER -------- %
% ------------------------------ %

% title page w/o page numbers

\title{Automation and Webscraping With Python}
\author{Charles Clayton}
\date{\today}
\maketitle

\thispagestyle{empty}

\pagenumbering{roman}
\setcounter{page}{0}

% auto-generated front matter w/ roman page numbering

\singlespacing			\pagebreak
\tableofcontents		\pagebreak


% ----------------------------- %
% -------- MAIN MATTER -------- %
% ----------------------------- %

\pagenumbering{arabic}
\onehalfspacing

\section{Scraping with \texttt{Selenium}}


Welcome to the first volume in our course Getting Started with Python Web Scraping. This volume is called Scraping with Selenium.

I assure you we will cover selenium, but before we do so we will first discuss circumstances in which you might want to web scrape and why it's a valuable skill.

We will spend a good amount of time first covering how websites work and what they're made of. Without knowing this, it's impossible to write code to interact with them for you. After that we will  install a webdriver, the tool needed to webscrape with selenium.

We will then walk though a basic example of webscraping, and I will discuss the technical aspects and the thought process as we go.

Finally we will conclude by discussing the ethics and legality of webscraping. And your responsibilities as a programmer and as a person.

\subsection{When to Use Web Scraping}

Let's say we need some data. Maybe it's sports statistics, maybe it's job listings, maybe you want to  compare prices across certain competitors, the main take away from this course is that there's usually a more efficient way to get this data than doing it manually.

We might either want to write a program to repeatedly retrieve the data in the case that it's being updated, or it might just be way too tedious for us to copy and paste each relevant piece of data by hand into an excel spreadsheet or what have you. This is where  webscraping comes in. This tool will allow us to automate the process of gathering data from the internet and be more productive in our day-to-day personal and professional lives. 

After familiarizing yourself with webscraping, it also shapes how you view websites and the internet in general. You can see it as an easier source of data, which means you're more likely to be interested in accumulating the information and doing some of your own comparisons, analysis, or visualizations of it. This leads to making more informed and numerical decisions in the long run.

So, it's a really cool tool and a valuable skill to learn regardless of your field of work.

\subsubsection{Before you Begin}

Before we begin, this course assumes that you already have some foundation in Python programming, or even just programming in general. We also assume that you have Python installed, and that you know how to install Python modules using pip.

I will be using Python 3.4 for this section, and I recommend you use the same version. I will also be using a Windows 10 PC, but the process will be the same except for maybe some syntax in the events that we might be saving or loading files.


The first method of webscraping we will go over is using the Selenium module. Selenium is the best framework to start with because it deals immediately with the browser which everyone is familiar with, and the interactions happen tangibly in front of you. 


\subsection{What Makes Up A Website}

In order to scrape data from a website, we have to first understand what makes up a website and how data on a website is organized. Once we know this, we can tell our script the location of the data we're interested in so that it can grab it for us. 


A web page is made out of three core elements: \textbf{HTML}, \textbf{CSS}, and \textbf{JavaScript}. HTML is mostly in charge of the content and structure of the website -- like all the text and images, and the different parts of a page. 

\begin{verbatim}
<h1>Hello, World!</h1>
\end{verbatim}

 CSS is in charge of the style of the website and layout of the website -- such as the colours, fonts, and what content is next to or overtop of what. 
 
 \begin{verbatim}
 h1 {
  color:red;
  text-align:center;
}
 \end{verbatim}
 
 JavaScript is in charge of the interactivity of a webpage -- popups, timeouts, reactions to clicks and mouse movements, and so on. 
 
\begin{verbatim}
document.onclick = () =>
  alert("You clicked!");
 \end{verbatim} 
 
This is a good overview, but in reality the functionality of these different languages can overlap somewhat. For instance both CSS and JavaScript can be used to create animations; both CSS and HTML can be used to enforce page layout; and interactivity can be achieved with different kinds of languages called server-side languages, like C\#, PHP, and Python, which we will discuss later.



\subsubsection{Using the Chrome Developer tools}

To explore these elements, you can use your browser's developer tools. In this course, we will be using the Chrome browser -- but all modern browsers will have analogous functionality.

To open developer tools, right-click the page and choose the \textbf{Inspect} context-menu item. Alternatively, you can press \textbf{F12}. Let's poke around a website and see some examples of HTML, CSS, and JavaScript in action.

\url{https://github.com/crclayton/invoicing/commits/master}.

\subsubsection{HTML Tags}

So here we can see that HTML is structured into \textbf{tags}, that look like so: 

 \verb|<tag>content</tag>|.
 
 Where \texttt{<tag>} begins the content, and \verb|</tag>| with the forward-slash ends the content within.
  
Some common tags are \texttt{p} for paragraph, \texttt{div} for page division, \texttt{h1, h2, etc.} for headers, \texttt{a} for links, and \texttt{li} for list items. Even without any styling, a browser will look at these tags and make educated guesses for how to display the content within them. 

Here's a comprehensive list of  the tags a website may have: \url{http://www.w3schools.com/tags/default.asp}.

Tags also have attributes, also called properties. These just specify more information about the tag other than its content. For instance, a \texttt{input} tag has an attribute which can specify whether it will input a date, a password, plain-text, just a check-mark, and so on.

\url{https://codepen.io/pen/}.

\begin{verbatim}
<input type="text" value="Input"\>
\end{verbatim}

Different tags have different attributes, but the most common attributes are the \textbf{id} and the \textbf{class} attributes. An id is unique to a tag, and allows it to be directly identified. 

\begin{verbatim}
.demo {
  background:lime;
  font-size:large;
}
\end{verbatim}

A class isn't unique, and is usually used to assign a common CSS style to all tags of that class. 

\subsubsection{CSS Classes}

Tags can have multiple classes. The helpful thing about classes when webscraping isn't that they identify common styles, it's that we can use them to locate the data we want. If there are repeating tags containing the data we want, we can be fairly confident they will have a common class. 

For instance, take a look at this GitHub commit log.

\url{https://github.com/Microsoft/TypeScript}. 

If I was interested in downloading a record of all the commit messages and who posted them, I could see that all messages all within an \texttt{a} tag and all authors are stored in a \texttt{span} tag. However, there are lots of other tags that are links  and spans on this page that aren't commit messages, so we have to be more precise.

But since all the commit messages look the same, and all the authors look the same, we know that they probably have the same class. 

And sure enough, each link with a commit message has a class \texttt{message}, and each commit author has a class \texttt{commit-author}.

Now we can use this to go through the page and gather the data in each tag with those classes.

\subsubsection{JavaScript Queries}

We can locate the tags we want using JavaScript queries and CSS selectors. 

I'm going to use JavaScript in the developer tools console to demonstrate some queries. 

Although we won't be using JavaScript when we do our scraping, the way we identify the tags we want is the same in both Python and JavaScript, and it is faster to debug to make sure we are getting exactly what it is we want within the browser.

Using JavaScript, we can call the method \texttt{document.getElementsByClassName()}. and this will return a list of all those tags. You can see the other types of queries using the ID or another attribute called the Name.

If you expand out the list, you can hover over the items and see their corresponding location on the page.

\subsubsection{CSS Selectors}

A more powerful technique for these queries is using something called CSS selectors. 

If you want to find something by the tag, just enter the tag. If you want to find something by a class name, enter the class preceded with a period. If you want to find something by the id, enter the id preceded by a pound-sign. 

For instance

\verb|document.querySelector("#fork-destination-box")|

and 

\verb|document.getElementById("fork-destination-box")|

Perform the same function, however, with CSS selectors we can nest queries more easily, specifying tags of a certain class within a tag with a certain ID, with an attribute that equals a certain value.


Consider the following example. Here we identify the tag by the id, then identify a tag within that tag, one of its descendants, by the class, then identify a tag within that tag by the tag type. 

\begin{verbatim}
document.querySelector("#fork-destination-box .fork-select-fragment img")
\end{verbatim}

Here's a comprehensive list of CSS selectors. The more adept you are at using these queries to precisely identify the data you want, the more streamlined and elegant your scraping can be.

\url{http://www.w3schools.com/cssref/css_selectors.asp}

\subsubsection{XPaths}

When all else fails. If there are no ids, no names, no classes, no attributes we can use. You may be able to use an XPath, which identifies a tag by its position in the HTML hierarchy. These aren't ideal because if a website makes a small change like adding or removing an element, this query may be broken. Whereas style and tag selectors are more robust. 

For now, that will be all the web-development we will cover. However, I cannot stress enough the importance of familiarizing yourself with these queries as it can greatly simplify your code from code that looks something like this:

\begin{verbatim}
images = []
for e in browser.find_elements_by_class_name("example"):
    for i in e.find_elements_by_tag_name("img"):
        if "foo" in i.get_attribute("alt"):
            images.append(i)
\end{verbatim}

To code that looks something like this:

\begin{verbatim}
images = browser.find_element_by_css_selector(".example > img[alt~=foo]")
\end{verbatim}



\subsection{Using the Selenium Module}

Okay, I know that seems like a whole lot of fuss about nothing relevant so far. This is supposed to be about Python webscraping and so far we haven't touched Python, it's all been this web-development stuff. But now we'll start to use Python and you'll see the exact same techniques will apply, and that knowing how websites work and how they are built will help us immensely with our scraping.

\subsubsection{Setting Up A WebDriver}

So first we'll install our WebDriver. A WebDriver is commonly used as a tool for testing your web-applications. If you build an app, instead of clicking every single button and making sure it all works correctly, you can write code to simulate all sorts of clicks and entries into the browser and make sure it doesn't break.

For us, this is a handy tool that lets us write Python code to tell a browser what to do, and more importantly, grab the contents of a website.

For the WebDriver in this tutorial, we'll be using ChromeDriver which uses the Chrome browser and can be downloaded here: \url{https://sites.google.com/a/chromium.org/chromedriver/}. 

Download the \texttt{.exe} and we'll put it in our project directory. Let's also create a Python file there, and we're going to be using Python 3.4. 

We can now use our Python code to take hold of the browser with the \texttt{selenium} module like so:

\begin{verbatim}
import selenium
webdriver.Chrome("chromedriver.exe")
\end{verbatim}

Now let's assign this to a variable called \texttt{browser}. We'll use this to start scraping in just a moment, but let's scroll through the autocomplete options to get an idea of what we can do with our browser now.

We can navigate to a website like so:

 \texttt{browser.get("https://google.com")}

We can also insert any JavaScript code we want to run:

\begin{verbatim}
browser.execute_script("alert('hello')")
\end{verbatim}

We can modify cookies, go back or forward or refresh, or even take screenshots of the current page if we want:

\begin{verbatim}
browser.save_screenshot("test.png")
\end{verbatim}

And most importantly, we have those element finder queries we discussed before. Using these we can select elements and mess around with them. Let's select the google input textbox.

\begin{verbatim}
input = browser.find_element_by_css_selector("#lst-ib")
\end{verbatim}

And again using autocomplete, I just want to show you what we can do with an element. Here we can use \texttt{input.click()} and you can see the cursor focuses there. We can also child elements, that is tags within the selected tag. We can also simulate text input with the \verb|input.send_keys("example text")| method. Then clear the textbox with \verb|input.clear()|.

So, we have quite a bit of power to tell the browser what to do. Now let's tell it to get some data for us!

\subsubsection{Automating the Browser}

Let's start by going to \url{https://reddit.com}. Perhaps I want to start by getting a list of all the posts on the front page. If I inspect element, can see that it has this class \texttt{title}, so maybe we start by using that.

\begin{verbatim}
titles = browser.find_elements_by_css_selector(".title")
\end{verbatim}

Let's take a look at the text in all those elements using Python

\begin{verbatim}
[t.text for t in titles]
\end{verbatim}

Hmm, that's not exactly what we want. It looks like other elements have the class \texttt{title} so that's messing up our query. Let's try to be a bit more precise. 



\subsubsubsection{Debugging with the Console}

At this point, it's easier to switch to the developer console to debug so let's try that.

Recall that Python's \verb|browser.find_elements_by_css_selector| and JavaScript's \verb|document.querySelectorAll| do the exact same thing. So let's try 

\begin{verbatim}
document.querySelectorAll(".title") 
\end{verbatim}

And see what we get. It looks like this element at the top here has that class, as well as these elements at the bottom. Furthermore, it looks like there is both a paragraph and an anchor tag with that class for each link, so we're getting doubles. We can refine our query by specifying we only want the links by putting an \texttt{a} in our queryselector followed by the class type.

There we go, now we only have the links, so we can get the text from those more easily. But let's take it a step further. It looks like this class \verb|outbound| specifies whether the link goes to another website or stays on reddit.com. Let's say we only want external links. We can add another class to our selector and that refines our list even more.


\begin{verbatim}
document.querySelectorAll("a.title.outbound") 
\end{verbatim}

Now you might be thinking, well, if we can do this all in the browser, why even use Python. But the problem with this JavaScript, is we can't continue to run the code across page changes. So let's say we want to keep getting the text from these posts across to the next page, we can do that with our Python script.

Let's move our Python code into a working script now:

\begin{verbatim}
import selenium 

browser = selenium.webdriver.Chrome("chromedriver.exe")
browser.get("https://reddit.com")
titles = browser.find_elements_by_css_selector("a.title.outbound")
titlesText = [t.text for t in titles]
\end{verbatim}

Let's say we want to get the text for the first three pages. We'll now tell the browser to press the  next page button and repeat our code.

Let's look at the next button, it has the class \verb|next-button| and a quick test shows us that no other elements have that class. So instead of doing \verb|find_elements|, which returns a list, let's use \verb|find_element| which will return the first element it finds. In this case it'll be the only one.

And we'll call the \verb|click| method on that element.

\begin{verbatim}
next = browser.find_element_by_css_selector(".next-button")
next.click()
\end{verbatim}

Now let's run the full script, getting the posts of the first three pages.

\begin{verbatim}
from selenium import webdriver

browser = webdriver.Chrome("chromedriver.exe")
browser.get("https://reddit.com")

postTitles = []

for i in range(3):
    titles = browser.find_elements_by_css_selector("a.title.outbound")
    postTitles +=  [t.text for t in titles]

    next = browser.find_element_by_css_selector(".next-button")
    next.click()

for post in postTitles:
    print(post)
\end{verbatim}

One problem you might get depending on your console is that it could stuggle with some unicode characters. If that's the case, don't worry because Python 3 is handling it and getting the data, it's just that your console doesn't support the characters. For instance, my visual studio console here doesn't have a problem printing it, but the default console doesn't know how to decode the characters.

If you just want to print, you can try using this code:

\begin{verbatim}
import sys
...
print(post.encode(sys.stdout.encoding, errors='replace'))
\end{verbatim}

We'll discuss encoding in more detail in later videos.

\subsection{Ethics}

This leads us to a point where we should discuss the ethics of webscraping. It's important to remember that people run these websites, and that it costs them money to service requests. We only went through five pages, but reddit was forced to service far more data faster than it expects from a typical user. Imagine if we had decided to run our script across the first 100 pages -- this would an aggressive scraper and we would be imposing an irresponsibly large workload on reddit, disrupting their business.

First and foremost, this is unkind. Case closed.

Beyond that, some scraping is against the terms and conditions of certain websites. If you're found to be scraping aggressively, a website can easily detect this and you are likely to find yourself blocked from certain websites. The simplest way to prevent this, is to impose delays in your script. Sure, it'll take longer, but run it at night and you won't notice any difference and you won't be hogging the throughput of the website. Moreover, you won't find yourself blocked and your scraper broken. 

Finally, you aren't going to find yourself coding around captchas.

The law revolving around webscraping is complicated, varies by country, is somewhat subjective, and challenging to enforce. The best advice is to be considerate, imagine you were the one running the website, and use common sense.

Where possible, using an API -- an application program interface -- is a wise alternative to traversing the HTML. These are mechanisms by which a website will allow you to make queries directly to the server, without using the middle-man as a browser.

We will examine APIs in a later section.

\subsubsection{Using a Headless Browser (PhantomJS)}

At this point, some programmers may feel uncomfortable with the browser popping up on the screen when they run their script. It would be nice if this could be done as an invisible, background task instead of hogging the computer while you could be doing something else.

I recommend writing your script using Chrome to debug, but once you have a working script. You may consider using a tool such as PhantomJS. PhantomJS is a browser that we can control with our webdriver, however it's headless -- that is, invisible. 


\section{Parsing with \texttt{BeautifulSoup 4}}

\subsection{Requesting HTML}

\subsubsection{\texttt{urllib2} preview}

\subsubsection{\texttt{Requests}}

\subsubsection{\texttt{wget}}

\subsection{\texttt{BeautifulSoup} Objects}

\subsection{\texttt{BeautifulSoup} Queries}

\subsubsection{Comparison To JS/Selenium Queries}

\subsubsection{Nested Selectors}

\subsubsection{Regular Expressions Basics}

\subsection{Inevitable Roadblocks}

\subsubsection{Server-side vs. Client-side Websites}

\subsubsection{UTF-8 Encoding}

\subsubsection{Login Pages}

\section{Fetching with \texttt{urlib2}}

\subsection{Using the Developer Tools Network Tab}

\subsection{Bypassing the Browser}

\subsubsection{URL Query Strings}

\subsubsection{HTTP Requests}

\subsubsubsection{\texttt{GET}}

\subsubsubsection{\texttt{POST}}

\subsubsection{Retrieving Files}

\subsubsection{Demo with WireShark}

\subsection{Authentication}

\subsection{Refused/Timed out Connections}



\section{Beginning New Projects}

\subsection{Responsible Scraping}

\subsection{Defining the Goal}

\subsection{Identifying the Obstacles}

\subsection{Selecting the Right Tool}

\subsection{Avoiding Pitfalls}

\section{Example Project Demo}

\subsection{Accomplishing our Goal With \texttt{Selenium}}

\subsubsection{Walkthrough of Process}

\subsubsection{Strengths and Limitations}


\subsection{Accomplishing our Goal With \texttt{BeautifulSoup}}

\subsubsection{Walkthrough of Process}

\subsubsection{Strengths and Limitations}


\subsection{Accomplishing our Goal With \texttt{urllib2}}

\subsubsection{Walkthrough of Process}

\subsubsection{Strengths and Limitations}




\end{document}