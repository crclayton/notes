\documentclass{article}
\usepackage{array, booktabs, graphicx, apacite, setspace, tocbibind, lipsum}
\usepackage[titletoc,toc,title]{appendix}
\usepackage[toc,section=section]{glossaries}
\usepackage[parfill]{parskip}

% format appendix numbering
\renewcommand\appendix{\par
  \setcounter{section}{0}
  \setcounter{subsection}{0}
  \setcounter{figure}{0}
  \setcounter{table}{0}
  \renewcommand\thesection{Appendix \Alph{section}}
  \renewcommand\thefigure{\Alph{section}\arabic{figure}}
  \renewcommand\thetable{\Alph{section}\arabic{table}}
}

% bold typewriter
\usepackage{lmodern}
\ttfamily
\fontseries{b}\selectfont

% renames "Contents" to "Table of Contents"
\renewcommand\contentsname{Course Overview}


% define subsubsubsection
\usepackage{titlesec}
\usepackage{hyperref}


\titleclass{\subsubsubsection}{straight}[\subsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}



\usepackage[top=1.5in, bottom=1.9in, left=1.75in, right=1.75in]{geometry}


\begin{document}

% ------------------------------ %
% -------- FRONT MATTER -------- %
% ------------------------------ %

% title page w/o page numbers

\title{Automation and Webscraping With Python}
\author{Charles Clayton}
\date{\today}
\maketitle

\thispagestyle{empty}

\pagenumbering{roman}
\setcounter{page}{0}

% auto-generated front matter w/ roman page numbering

\singlespacing			\pagebreak
\tableofcontents		\pagebreak


% ----------------------------- %
% -------- MAIN MATTER -------- %
% ----------------------------- %

\pagenumbering{arabic}
\onehalfspacing

\section{Scraping with \texttt{Selenium}}


Welcome to the first volume in our course Getting Started with Python Web Scraping. This volume is called Scraping with Selenium.

I assure you we will cover selenium, but before we do so we will first discuss circumstances in which you might want to web scrape and why it's a valuable skill.

We will spend a good amount of time first covering how websites work and what they're made of. Without knowing this, it's impossible to write code to interact with them for you. After that we will  install a webdriver, the tool needed to webscrape with selenium.

We will then walk though a basic example of webscraping, and I will discuss the technical aspects and the thought process as we go.

Finally we will conclude by discussing the ethics and legality of webscraping. And your responsibilities as a programmer and as a person.

\subsection{When to Use Web Scraping}

Let's say we need some data. Maybe it's sports statistics, maybe it's job listings, maybe you want to  compare prices across certain competitors, the main take away from this course is that there's usually a more efficient way to get this data than doing it manually.

We might either want to write a program to repeatedly retrieve the data in the case that it's being updated, or it might just be way too tedious for us to copy and paste each relevant piece of data by hand into an excel spreadsheet or what have you. This is where  webscraping comes in. This tool will allow us to automate the process of gathering data from the internet and be more productive in our day-to-day personal and professional lives. 

After familiarizing yourself with webscraping, it also shapes how you view websites and the internet in general. You can see it as an easier source of data, which means you're more likely to be interested in accumulating the information and doing some of your own comparisons, analysis, or visualizations of it. This leads to making more informed and numerical decisions in the long run.

So, it's a really cool tool and a valuable skill to learn regardless of your field of work.

\subsubsection{Before you Begin}

Before we begin, this course assumes that you already have some foundation in Python programming, or even just programming in general. We also assume that you have Python installed, and that you know how to install Python modules using pip.

I will be using Python 3.4 for this section, and I recommend you use the same version. I will also be using a Windows 10 PC, but the process will be the same except for maybe some syntax in the events that we might be saving or loading files.


The first method of webscraping we will go over is using the Selenium module. Selenium is the best framework to start with because it deals immediately with the browser which everyone is familiar with, and the interactions happen tangibly in front of you. 


\subsection{What Makes Up A Website}

In order to scrape data from a website, we have to first understand what makes up a website and how data on a website is organized. Once we know this, we can tell our script the location of the data we're interested in so that it can grab it for us. 


A web page is made out of three core elements: \textbf{HTML}, \textbf{CSS}, and \textbf{JavaScript}. HTML is mostly in charge of the content and structure of the website -- like all the text and images, and the different parts of a page. 

\begin{verbatim}
<h1>Hello, World!</h1>
\end{verbatim}

 CSS is in charge of the style of the website and layout of the website -- such as the colours, fonts, and what content is next to or overtop of what. 
 
 \begin{verbatim}
 h1 {
  color:red;
  text-align:center;
}
 \end{verbatim}
 
 JavaScript is in charge of the interactivity of a webpage -- popups, timeouts, reactions to clicks and mouse movements, and so on. 
 
\begin{verbatim}
document.onclick = () =>
  alert("You clicked!");
 \end{verbatim} 
 
This is a good overview, but in reality the functionality of these different languages can overlap somewhat. For instance both CSS and JavaScript can be used to create animations; both CSS and HTML can be used to enforce page layout; and interactivity can be achieved with different kinds of languages called server-side languages, like C\#, PHP, and Python, which we will discuss later.



\subsubsection{Using the Chrome Developer tools}

To explore these elements, you can use your browser's developer tools. In this course, we will be using the Chrome browser -- but all modern browsers will have analogous functionality.

To open developer tools, right-click the page and choose the \textbf{Inspect} context-menu item. Alternatively, you can press \textbf{F12}. Let's poke around a website and see some examples of HTML, CSS, and JavaScript in action.

\url{https://github.com/crclayton/invoicing/commits/master}.

\subsubsection{HTML Tags}

So here we can see that HTML is structured into \textbf{tags}, that look like so: 

 \verb|<tag>content</tag>|.
 
 Where \texttt{<tag>} begins the content, and \verb|</tag>| with the forward-slash ends the content within.
  
Some common tags are \texttt{p} for paragraph, \texttt{div} for page division, \texttt{h1, h2, etc.} for headers, \texttt{a} for links, and \texttt{li} for list items. Even without any styling, a browser will look at these tags and make educated guesses for how to display the content within them. 

Here's a comprehensive list of  the tags a website may have: \url{http://www.w3schools.com/tags/default.asp}.

Tags also have attributes, also called properties. These just specify more information about the tag other than its content. For instance, a \texttt{input} tag has an attribute which can specify whether it will input a date, a password, plain-text, just a check-mark, and so on.

\url{https://codepen.io/pen/}.

\begin{verbatim}
<input type="text" value="Input"\>
\end{verbatim}

Different tags have different attributes, but the most common attributes are the \textbf{id} and the \textbf{class} attributes. An id is unique to a tag, and allows it to be directly identified. 

\begin{verbatim}
.demo {
  background:lime;
  font-size:large;
}
\end{verbatim}

A class isn't unique, and is usually used to assign a common CSS style to all tags of that class. 

\subsubsection{CSS Classes}

Tags can have multiple classes. The helpful thing about classes when webscraping isn't that they identify common styles, it's that we can use them to locate the data we want. If there are repeating tags containing the data we want, we can be fairly confident they will have a common class. 

For instance, take a look at this GitHub commit log.

\url{https://github.com/Microsoft/TypeScript}. 

If I was interested in downloading a record of all the commit messages and who posted them, I could see that all messages all within an \texttt{a} tag and all authors are stored in a \texttt{span} tag. However, there are lots of other tags that are links  and spans on this page that aren't commit messages, so we have to be more precise.

But since all the commit messages look the same, and all the authors look the same, we know that they probably have the same class. 

And sure enough, each link with a commit message has a class \texttt{message}, and each commit author has a class \texttt{commit-author}.

Now we can use this to go through the page and gather the data in each tag with those classes.

\subsubsection{JavaScript Queries}

We can locate the tags we want using JavaScript queries and CSS selectors. 

I'm going to use JavaScript in the developer tools console to demonstrate some queries. 

Although we won't be using JavaScript when we do our scraping, the way we identify the tags we want is the same in both Python and JavaScript, and it is faster to debug to make sure we are getting exactly what it is we want within the browser.

Using JavaScript, we can call the method \texttt{document.getElementsByClassName()}. and this will return a list of all those tags. You can see the other types of queries using the ID or another attribute called the Name.

If you expand out the list, you can hover over the items and see their corresponding location on the page.

\subsubsection{CSS Selectors}

A more powerful technique for these queries is using something called CSS selectors. 

If you want to find something by the tag, just enter the tag. If you want to find something by a class name, enter the class preceded with a period. If you want to find something by the id, enter the id preceded by a pound-sign. 

For instance

\verb|document.querySelector("#fork-destination-box")|

and 

\verb|document.getElementById("fork-destination-box")|

Perform the same function, however, with CSS selectors we can nest queries more easily, specifying tags of a certain class within a tag with a certain ID, with an attribute that equals a certain value.


Consider the following example. Here we identify the tag by the id, then identify a tag within that tag, one of its descendants, by the class, then identify a tag within that tag by the tag type. 

\begin{verbatim}
document.querySelector("#fork-destination-box .fork-select-fragment img")
\end{verbatim}

Here's a comprehensive list of CSS selectors. The more adept you are at using these queries to precisely identify the data you want, the more streamlined and elegant your scraping can be.

\url{http://www.w3schools.com/cssref/css_selectors.asp}

\subsubsection{XPaths}

When all else fails. If there are no ids, no names, no classes, no attributes we can use. You may be able to use an XPath, which identifies a tag by its position in the HTML hierarchy. These aren't ideal because if a website makes a small change like adding or removing an element, this query may be broken. Whereas style and tag selectors are more robust. 

For now, that will be all the web-development we will cover. However, I cannot stress enough the importance of familiarizing yourself with these queries as it can greatly simplify your code from code that looks something like this:

\begin{verbatim}
images = []
for e in browser.find_elements_by_class_name("example"):
    for i in e.find_elements_by_tag_name("img"):
        if "foo" in i.get_attribute("alt"):
            images.append(i)
\end{verbatim}

To code that looks something like this:

\begin{verbatim}
images = browser.find_element_by_css_selector(".example > img[alt~=foo]")
\end{verbatim}



\subsection{Using the Selenium Module}

Okay, I know that seems like a whole lot of fuss about nothing relevant so far. This is supposed to be about Python webscraping and so far we haven't touched Python, it's all been this web-development stuff. But now we'll start to use Python and you'll see the exact same techniques will apply, and that knowing how websites work and how they are built will help us immensely with our scraping.

\subsubsection{Setting Up A WebDriver}

So first we'll install our WebDriver. A WebDriver is commonly used as a tool for testing your web-applications. If you build an app, instead of clicking every single button and making sure it all works correctly, you can write code to simulate all sorts of clicks and entries into the browser and make sure it doesn't break.

For us, this is a handy tool that lets us write Python code to tell a browser what to do, and more importantly, grab the contents of a website.

For the WebDriver in this tutorial, we'll be using ChromeDriver which uses the Chrome browser and can be downloaded here: \url{https://sites.google.com/a/chromium.org/chromedriver/}. 

Download the \texttt{.exe} and we'll put it in our project directory. Let's also create a Python file there, and we're going to be using Python 3.4. 

We can now use our Python code to take hold of the browser with the \texttt{selenium} module like so:

\begin{verbatim}
import selenium
webdriver.Chrome("chromedriver.exe")
\end{verbatim}

Now let's assign this to a variable called \texttt{browser}. We'll use this to start scraping in just a moment, but let's scroll through the autocomplete options to get an idea of what we can do with our browser now.

We can navigate to a website like so:

 \texttt{browser.get("https://google.com")}

We can also insert any JavaScript code we want to run:

\begin{verbatim}
browser.execute_script("alert('hello')")
\end{verbatim}

We can modify cookies, go back or forward or refresh, or even take screenshots of the current page if we want:

\begin{verbatim}
browser.save_screenshot("test.png")
\end{verbatim}

And most importantly, we have those element finder queries we discussed before. Using these we can select elements and mess around with them. Let's select the google input textbox.

\begin{verbatim}
input = browser.find_element_by_css_selector("#lst-ib")
\end{verbatim}

And again using autocomplete, I just want to show you what we can do with an element. Here we can use \texttt{input.click()} and you can see the cursor focuses there. We can also child elements, that is tags within the selected tag. We can also simulate text input with the \verb|input.send_keys("example text")| method. Then clear the textbox with \verb|input.clear()|.

So, we have quite a bit of power to tell the browser what to do. Now let's tell it to get some data for us!

\subsubsection{Automating the Browser}

Let's start by going to \url{https://reddit.com}. Perhaps I want to start by getting a list of all the posts on the front page. If I inspect element, can see that it has this class \texttt{title}, so maybe we start by using that.

\begin{verbatim}
titles = browser.find_elements_by_css_selector(".title")
\end{verbatim}

Let's take a look at the text in all those elements using Python

\begin{verbatim}
[t.text for t in titles]
\end{verbatim}

Hmm, that's not exactly what we want. It looks like other elements have the class \texttt{title} so that's messing up our query. Let's try to be a bit more precise. 



\subsubsubsection{Debugging with the Console}

At this point, it's easier to switch to the developer console to debug so let's try that.

Recall that Python's \verb|browser.find_elements_by_css_selector| and JavaScript's \verb|document.querySelectorAll| do the exact same thing. So let's try 

\begin{verbatim}
document.querySelectorAll(".title") 
\end{verbatim}

And see what we get. It looks like this element at the top here has that class, as well as these elements at the bottom. Furthermore, it looks like there is both a paragraph and an anchor tag with that class for each link, so we're getting doubles. We can refine our query by specifying we only want the links by putting an \texttt{a} in our queryselector followed by the class type.

There we go, now we only have the links, so we can get the text from those more easily. But let's take it a step further. It looks like this class \verb|outbound| specifies whether the link goes to another website or stays on reddit.com. Let's say we only want external links. We can add another class to our selector and that refines our list even more.


\begin{verbatim}
document.querySelectorAll("a.title.outbound") 
\end{verbatim}

Now you might be thinking, well, if we can do this all in the browser, why even use Python. But the problem with this JavaScript, is we can't continue to run the code across page changes. So let's say we want to keep getting the text from these posts across to the next page, we can do that with our Python script.

Let's move our Python code into a working script now:

\begin{verbatim}
import selenium 

browser = selenium.webdriver.Chrome("chromedriver.exe")
browser.get("https://reddit.com")
titles = browser.find_elements_by_css_selector("a.title.outbound")
titlesText = [t.text for t in titles]
\end{verbatim}

Let's say we want to get the text for the first three pages. We'll now tell the browser to press the  next page button and repeat our code.

Let's look at the next button, it has the class \verb|next-button| and a quick test shows us that no other elements have that class. So instead of doing \verb|find_elements|, which returns a list, let's use \verb|find_element| which will return the first element it finds. In this case it'll be the only one.

And we'll call the \verb|click| method on that element.

\begin{verbatim}
next = browser.find_element_by_css_selector(".next-button")
next.click()
\end{verbatim}

Now let's run the full script, getting the posts of the first three pages.

\begin{verbatim}
from selenium import webdriver

browser = webdriver.Chrome("chromedriver.exe")
browser.get("https://reddit.com")

postTitles = []

for i in range(3):
    titles = browser.find_elements_by_css_selector("a.title.outbound")
    postTitles +=  [t.text for t in titles]

    next = browser.find_element_by_css_selector(".next-button")
    next.click()

for post in postTitles:
    print(post)
\end{verbatim}

One problem you might get depending on your console is that it could stuggle with some unicode characters. If that's the case, don't worry because Python 3 is handling it and getting the data, it's just that your console doesn't support the characters. For instance, my visual studio console here doesn't have a problem printing it, but the default console doesn't know how to decode the characters.

If you just want to print, you can try using this code:

\begin{verbatim}
import sys
...
print(post.encode(sys.stdout.encoding, errors='replace'))
\end{verbatim}

We'll discuss encoding in more detail in later videos.

\subsection{Ethics}

In the last video we saw that we can use selenium to automate the browser to load and interact with websites very quickly. However, by not being responsible with your webscrapers, your script can be harmful. This leads us to a point where we should discuss the ethics of webscraping. 

- In this video, we will discuss why it's important to be mindful of the workload your scraper is putting on a website.

- We will discuss the possible consequences of webscrcaping irresponsibly. 

- And we will touch on how to mitigate your webscraper's harm and alternative solutions to webscraping.

\textbf{Effects}

It's important to remember that people run these websites in good faith. First of all, it can cost them time and money to collect the data you are web scraping. If you are scraping non-public information for redistribution, you may be liable to copyright infringement claims.

On a more technical side, recall that it costs a business money to host a website and run servers to respond to HTTP requests. 



In our example, we only went through three pages, but reddit was forced to service far more data faster than it expects from a typical user. Imagine if we had decided to run our script across the first 100 pages -- this would an aggressive scraper and we would be imposing an irresponsibly large workload on the website's servers, disrupting their business.

Furthermore, many websites use ad revenue to pay for their services, if you're using an aggressive web scraper and ignoring the ads, this is the internet equivalent of walking into a cafe and without buying anything, taking all the free napkins and all the condiment packets.

First and foremost, this is unkind. Case closed.

\textbf{Consequences}

But beyond that, just like a cafe is likely to throw you out and ban you from the restaurant, a website is likely to take action against you.

Web scraping is against the terms and conditions of certain websites. If you're found to be scraping aggressively, a website can easily detect this and you are likely to find yourself blocked from certain websites.

If not blocked completely, a website may detect heavy traffic and confront you with CAPTCHAs, which ostensibly stands for Completely Automated Public Turing test to tell Computer and Humans Apart. It's extremely unlikely you will be able to circumvent one of these, and it will break your script.

\textbf{Suggestions}

The simplest way to avoid scraping aggressively is to impose delays in your script. Sure, it'll take longer, but if you run it at night and you won't notice any difference and you won't be hogging the throughput of the website. Moreover, you won't find yourself blocked and your scraper broken. It's also a good idea to run your script at off-peak hours when the website is experiencing lower traffic, such as nights or weekends.

Before you even write your script, search the website's terms and conditions for words such as "scrape", scraping, "mine", mining, "extract", extracting, and so on. If you're unsure, there is no harm in contacting a website's administrator or webmaster. They might even be able to provide you with the data you're trying to extract, or inform you of another solution that is less likely to put stress on the website.

For instance, using an API -- an application program interface. APIs are a wise alternative to webscraping if possible. These are mechanisms by which a website will allow you to make queries for data directly to the server, bypassing the need to use the browser and traverse the HTML at all.

We will examine APIs the volume fetching with urllib2.


The law revolving around webscraping is complicated, varies by country, and is challenging to enforce. The best advice is to be considerate, imagine you were the one running the website, and use common sense.


\textbf{Summary}

In this section, we covered when and why web scraping is a practical tool. 

We then looked at how websites are built and how data on them is structured in HTML, then we learned how to use CSS selectors to identify and gather specific data on a website, and how to develop the selectors with the developer tools.

We then introduced the selenium module, and we used Python to control a webdriver to gather data from a website. We will revisit selenium in more detail in the course Advanced Python Web Scraping.

Finally we discussed some strategies to web scrape responsibly and ethically.

\textbf{Next}

Now that we have a handle on web scraping using a browser, we're going to shift gears and focus on doing this all in Python with the BeautifulSoup module.

\subsubsection{Using a Headless Browser (PhantomJS)}

At this point, some programmers may feel uncomfortable with the browser popping up on the screen when they run their script. It would be nice if this could be done as an invisible, background task instead of hogging the computer while you could be doing something else.

I recommend writing your script using Chrome to debug, but once you have a working script. You may consider using a tool such as PhantomJS. PhantomJS is a browser that we can control with our webdriver, however it's headless -- that is, invisible. 


\section{Parsing with \texttt{BeautifulSoup 4}}

Now that we have seen some basic Python web-scraping using Selenium, it is time to look into more streamlined solutions. Instead of depending on external resources like webdrivers and parsing within the browser, which can be quite slow, we will now move the entire process to python, parsing the HTML using the BeautifulSoup module.

\subsection{Requesting HTML}

BeautifulSoup is only an HTML parser, it does not fetch the HTML from the website, so first we will have to look at methods of doing this. 


\subsubsection{\texttt{Requests}}

The \verb|requests| module makes this as quick and easy as possible.

\begin{verbatim}
url = r"https://en.wikipedia.org/wiki/List_of_battles_by_casualties"
response = requests.get(url)
\end{verbatim}

If you provided a valid URL, now probably now have the response HTML no problem. But it's a good idea to first make sure that there weren't any problems by checking the status code. You've probably all heard of the famous 404 response for when a web-site isn't found, but for a successful query, the response code is 200.

\begin{verbatim}
if response.status_code != 200:
    print("Failed to get HTML: ", response.status_code, response.reason)
    exit()    
\end{verbatim}

Other codes to be aware of are...

Also, to be transparent, I'm going to identify myself in the header of the request we're sending to this server. That way if the website owner gets unusual traffic from our webscraper they can determine the cause and the source.

\subsubsection{\texttt{urllib2} preview}

Another commonly used HTTP request module is \texttt{urllib}. In this tutorial, I'm using Python 3, but if you're using Python 2 the module will be \verb|urllib2|.

Similarly, a basic query for the HTML is the following:



\subsubsection{\texttt{wget}}

If instead of grabbing the HTML from a website in real-time during the execution of your script, you want to download an entire website then look at the HTML from files, a very common tool for this is \texttt{wget}.

\texttt{wget} can recursively download an entire website, downloading the provided page, then downloading all pages it links to and all the files that it depends on. 

Then you can use Python to load your HTML from the downloaded files like so:

Although \texttt{wget} can be quite aggressive and put a lot of demand on a website, if you're debugging a script that repeatedly loads lots of pages, it's probably a good idea to download the entire page once and then debug and re-run your script on local files.

\subsection{Parsers}

By default, BeautifulSoup uses the HTML parser that's native to Python's standard library, but you can use other parsers for different reasons.

Slightly  slower, but a much more forgiving HTML parser that emulates more how browsers operate is the \texttt{html5lib} library. Some websites have invalid HTML, and modern browsers are forgiving and pretty much figure out what it's supposed to be and add missing close tags and so on. We will which we can install using pip:

 \texttt{pip install html5lib}

\subsection{\texttt{BeautifulSoup} Objects}

\subsubsection{Tags}

Tags have names, which are properties. These have read and write permissions, but for the most part we will only be reading.

All tags have attributes, which are in a dictionary so you can access attributes like a normal python dictionary. However if the attributes tag doesn't have the given key, you'll get a keyerror, so it's safer better to use the \texttt{get()} method which will return a \texttt{None} if the key doesn't exist.

\subsection{\texttt{BeautifulSoup} Queries}

Since we've talked about using CSS queries, those are still the best tool for the job when querying using \texttt{BeautifulSoup}. You can still determine the queries you would like to use using the Chrome developer tools console, or by experimenting in a Python console, but it'll be a little less readable when parsing long HTML in the console.

We can use CSS selectors to select all tags matching the query by using \verb|soup.select()| and to select only the first match with \verb|soup.select_one()|.

This will prevent us writing code that looks like this: 

\begin{verbatim}
matches = []
divs = soup.find_all("div", recursive=False)
for d in divs: 
    if d.get("class") == "foo":
        matches.append(list(d.children)[0])
\end{verbatim}

And instead write code that looks like this:

\verb|matches = soup.select("div > .foo:first-child")|



\subsubsection{Regular Expressions Basics}

If you're a regular expression whiz, you can also use regular expressions as your queries using the \texttt{re} module like so:

\verb|soup.find_all(re.compile("..."))|
 
\verb|soup.find_all(id=re.compile(), class_=re.compile("..."))|

Note here that you can specify the attribute you're querying on with \verb|find_all|, but \verb|class| is a reserved keyword in Python, so you'll have to use \verb|_class|.

\subsection{What to do now}

Store it in a pandas dataframe. Output it to a CSV. Graph with matplotlib.

\subsection{Inevitable Roadblocks}

\begin{verbatim}
from bs4.diagnose import diagnose
data = open("bad.html").read()
diagnose(data)
\end{verbatim}

\subsubsection{Server-side vs. Client-side Websites}

Now, there are different kinds of websites. Server-side websites will generate the HTML that makes up the whole page over at the server, then it will send you the HTML already rendered.

Client-side websites will send you some HTML, but they'll also send you code that then runs on your computer in the browser, which creates the rest of the website.

When using selenium, this wasn't a problem because we're using a browser, so this browser will run the client-side code that generates the rest of the website and we can scrape it. But with these requests, we're grabbing the HTML directly from the server, so if there's extra code that's suppose to fill out the rest of the website, we won't see that.

To determine if a website is client-side or server-side, one of the easiest things to do is compare the source code and the elements of a website. In Chrome, right-click a page and press view-source, and then right-click and press inspect element and look at the whole body. If these things are totally different, then the website is probably running client-side code.

\subsubsection{UTF-8 Encoding}

\texttt{from bs4 import UnicodeDammit}

For quotes:
\verb|smart_quotes_to=|

For newlines:
\verb|UnicodeDammit.detwingle()|

\subsubsection{Login Pages}

\subsection{Demo}

Let's first look at: \url{https://en.wikipedia.org/wiki/List_of_battles_and_other_violent_events_by_death_toll}.

First thing we see is that the tables probably have this class \verb|.wikitable|. And there's seven of them.



\section{Fetching with \texttt{urlib2}}

\subsection{Using the Developer Tools Network Tab}

\subsection{Bypassing the Browser}

\subsubsection{URL Query Strings}

\subsubsection{HTTP Requests}

\subsubsubsection{\texttt{GET}}

\subsubsubsection{\texttt{POST}}

\subsubsection{Retrieving Files}

\subsubsection{Demo with WireShark}

\subsection{Authentication}

\subsection{Refused/Timed out Connections}



\section{Beginning New Projects}

\subsection{Responsible Scraping}

\subsection{Defining the Goal}

\subsection{Identifying the Obstacles}

\subsection{Selecting the Right Tool}

\subsection{Avoiding Pitfalls}

\section{Example Project Demo}

\subsection{Accomplishing our Goal With \texttt{Selenium}}

\subsubsection{Walkthrough of Process}

\subsubsection{Strengths and Limitations}


\subsection{Accomplishing our Goal With \texttt{BeautifulSoup}}

\subsubsection{Walkthrough of Process}

\subsubsection{Strengths and Limitations}


\subsection{Accomplishing our Goal With \texttt{urllib2}}

\subsubsection{Walkthrough of Process}

\subsubsection{Strengths and Limitations}




\end{document}